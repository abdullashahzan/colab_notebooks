{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "182gGKoVtowAkPSkB2CY9_hsyJz_dvX83",
      "authorship_tag": "ABX9TyOus8pisrc4PBtbcPNeR9tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdullashahzan/colab_notebooks/blob/main/TensorFlow_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Downloading NLTK**"
      ],
      "metadata": {
        "id": "5J0SWk5lYwzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "LQ7drMin2aAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing libraries and initializing json and lemmatizer**"
      ],
      "metadata": {
        "id": "635iGPXyYmD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "MHiY4wG7tM8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = json.loads(open('/content/intents.json').read())"
      ],
      "metadata": {
        "id": "zqgkgovKt8qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "-jurE8E3uHna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing data**"
      ],
      "metadata": {
        "id": "bmkVzKdHKnEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# words is the list of our vocabulary with each word appearing only once from our sentence\n",
        "# classes are the labels associated with the particular pattern\n",
        "# documents is a list of [patterns, tag]\n",
        "# ignore letters is used for cleaning input data\n",
        "words, classes, documents, ignore_letters = [], [], [], ['?','.',',',\"'\", '!']"
      ],
      "metadata": {
        "id": "hyTJ8686uUV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to convert our patterns (X) and tags (y) in numerical value so that the machine can read it\n",
        "for intent in data['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    # word_tokenize will convert 'how are you' to ['how', 'are', 'you']\n",
        "    word_list = nltk.word_tokenize(pattern)\n",
        "    words.extend(word_list)\n",
        "    # your document will look like this (['how', 'are', 'you'], 'greetings')\n",
        "    documents.append((word_list, intent['tags']))\n",
        "    if intent['tags'] not in classes:\n",
        "      classes.append(intent['tags'])"
      ],
      "metadata": {
        "id": "Af9c3-TuusM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# over here we are creating our vocabulary where each word has its own space and identitiy suppose 'hello' will have [0,0,0,1,0,0] and \n",
        "# 'goodbye' will have [0,1,0,0,0,0] and so on...\n",
        "# sort will put everything in ascending order and set will delete and repeated value\n",
        "words = sorted(set([lemmatizer.lemmatize(word.lower()) for word in words if word not in ignore_letters]))\n",
        "classes = sorted(set(classes))"
      ],
      "metadata": {
        "id": "610SRe4Aw51p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training is a similar list like documents except that everything in it is in form of 0's and 1's like ([0,0,0,1,0,0,0], [0,1,0])0\n",
        "training = []\n",
        "for document in documents: \n",
        "  # output list is for our classes / labels / tags\n",
        "  output = [0 for _ in range(len(classes))]\n",
        "  # bag is for converting sentences into 0 and 1 matrices\n",
        "  bag = []\n",
        "  # reading the patterns from documents [(['how', 'are', 'you'], 'greetings')]\n",
        "  # where document = (['how', 'are', 'you'], 'greetings')\n",
        "  # and document[0] = ['how', 'are', 'you']\n",
        "  word_patterns = document[0]\n",
        "  word_patterns = [lemmatizer.lemmatize(w.lower()) for w in word_patterns]\n",
        "  # looping through the vocabulary we created and converting 0 to 1 where the word_pattern matches the word from our vocabulary\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "  output[classes.index(document[1])] = 1\n",
        "  training.append([bag, output])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype='object')\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])"
      ],
      "metadata": {
        "id": "iWy_X8ezy_1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building Neural Network**"
      ],
      "metadata": {
        "id": "brLO_4tgUFn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense is a normal layer\n",
        "# dropout layer helps model from overfitting or underfitting\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(128, input_shape=(len(train_x[0]),), activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # since we only have 3 tags for now, softmax gives output as 0.2, 0.7, 0.1 meaning the second tag has highest probability \n",
        "    tf.keras.layers.Dense(len(train_y[0]), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "2Jc69sYd3QMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vg1syqWs4_EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_x, train_y, epochs=200)"
      ],
      "metadata": {
        "id": "-PZPcZBR6KyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Program to use the model**"
      ],
      "metadata": {
        "id": "xlj-O5ApUP_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sentence(sentence):\n",
        "  sentence_words = nltk.word_tokenize(sentence)\n",
        "  sentence_words = [lemmatizer.lemmatize(w) for w in sentence_words]\n",
        "  return sentence_words"
      ],
      "metadata": {
        "id": "yFIWU8M7CdYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_bagofwords(sentence):\n",
        "  sentence_words = clean_sentence(sentence)\n",
        "  bag = [0 for _ in range(len(words))]\n",
        "\n",
        "  for w in sentence_words:\n",
        "    for i, word in enumerate(words):\n",
        "      if word == w:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)"
      ],
      "metadata": {
        "id": "wztwPqTnC6zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(sentence):\n",
        "  bagofwords = convert_bagofwords(sentence)\n",
        "  result = model.predict(np.array([bagofwords]))[0]\n",
        "  ERROR_THRESHOLD = 0.25\n",
        "  final_result = [[index, result] for index, result in enumerate(result) if result > ERROR_THRESHOLD]\n",
        "  final_result.sort(reverse=True)\n",
        "  return_list = []\n",
        "  for r in final_result:\n",
        "    return_list.append({'intent': classes[r[0]], 'probability': str(r[1])})\n",
        "  return return_list\n"
      ],
      "metadata": {
        "id": "fFJC0lH6D0K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_response(intents_list, intents_json):\n",
        "  tag = intents_list[0]['intent']\n",
        "  list_of_intents = intents_json['intents']\n",
        "  for i in list_of_intents:\n",
        "    if i['tags'] == tag:\n",
        "      result = random.choice(i['responses'])\n",
        "      break\n",
        "  return result"
      ],
      "metadata": {
        "id": "dzalESYSH9MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Talk to the chatbot**"
      ],
      "metadata": {
        "id": "L4oxtr6uYbxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "end = False\n",
        "while end != True:\n",
        "  ui = input(\"Type here: \")\n",
        "  if ui == \"stop\":\n",
        "    break\n",
        "  else:\n",
        "    intentsofui = predict_class(ui)\n",
        "    respond = get_response(intentsofui, data)\n",
        "    print(respond)"
      ],
      "metadata": {
        "id": "InvUpJkxGfCr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}